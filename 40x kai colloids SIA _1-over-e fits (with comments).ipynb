{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from numpy.fft import fft2, ifft2, fftshift\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import scipy\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.ndimage import gaussian_filter1d as gf1d\n",
    "from scipy.ndimage import gaussian_filter as gf\n",
    "from scipy.ndimage import uniform_filter as uf\n",
    "from skimage.transform import downscale_local_mean #For binning\n",
    "\n",
    "import xarray as xr #package for labeling and adding metadata to multi-dimensional arrays\n",
    "\n",
    "import sys\n",
    "#sys.path.append(\"../kai_colloids/PyDDM\") #must point to the PyDDM folder\n",
    "#import ddm_analysis_and_fitting as ddm   \n",
    "\n",
    "import tiff_file \n",
    "\n",
    "import io \n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import glob #glob is helpful for searching for filenames or directories\n",
    "import pickle #for saving data\n",
    "### usually this block prints out \"nd2reader module not found. Reading of .nd2 files disabled.\" on the first run\n",
    "### this is fine (unless you need to read .nd2 files), just re-run this block to make the error go away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First define the functions we will need to use\n",
    "### The Structural Image Autocorrelation (SIA) function is the second function defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this function (newRadav) finds the radial average of the image autocorrelation in the SIA function \n",
    "def newRadav(im, limangles=False, angRange=None, mask=None, rev=False,\n",
    "             debug_q = None):\n",
    "    if mask is None:\n",
    "        hasMask = False\n",
    "    else:\n",
    "        hasMask = True\n",
    "    nx,ny = im.shape\n",
    "    xx = np.arange(-(nx-1)/2., nx/2.)\n",
    "    yy = np.arange(-(ny-1)/2., ny/2.)\n",
    "    #x,y = np.meshgrid(xx,yy)\n",
    "    x,y = np.meshgrid(yy,xx)\n",
    "    q = np.sqrt(x**2 + y**2)\n",
    "    angles = np.arctan2(x,y)\n",
    "    \n",
    "    qx = np.arange(-1*nx/2,nx/2)*(1./nx) * max(nx,ny)\n",
    "    qy = np.arange(-1*ny/2,ny/2)*(1./ny) * max(nx,ny)\n",
    "    qxx,qyy = np.meshgrid(qy,qx) #qy,qx is correct order\n",
    "    q_new = np.sqrt(qxx**2 + qyy**2)\n",
    "    \n",
    "    if debug_q is not None:\n",
    "        return q_new.round().astype(int)==debug_q\n",
    "    \n",
    "    if mask is None:\n",
    "        mask = np.ones_like(angles)\n",
    "    if angRange is not None:\n",
    "        w1 = np.where(angles>angRange[0])\n",
    "    else:\n",
    "        w1 = np.where(angles>(13*np.pi/14))\n",
    "    if mask is None:\n",
    "        mask[w1]=0\n",
    "        mask = mask * np.rot90(np.rot90(mask))\n",
    "        mask = mask * np.flipud(mask)\n",
    "        mask[np.where(mask==0)] = np.nan\n",
    "        if rev:\n",
    "            mask = np.rot90(mask)\n",
    "    qr = q_new.round().astype(int)\n",
    "    #rs = np.arange(0,(nx-1)/2)\n",
    "    rs = np.arange(0,(max(nx,ny)-1)/2) \n",
    "    radav = np.zeros((len(rs)),dtype=float)\n",
    "    for i in range(0,len(rs)):\n",
    "        w = np.where(qr==rs[i])\n",
    "        if len(w[0])>0:\n",
    "            if limangles or hasMask:\n",
    "                newim = im*mask\n",
    "                radav[i] = np.nanmean(newim[w])\n",
    "            else:\n",
    "                radav[i] = np.nanmean(im[w])\n",
    "        #else:\n",
    "        #    print i\n",
    "    return radav\n",
    "\n",
    "### the SIA function \n",
    "def SIA(image, filter=True, filtersize=600, bin=True, binsize=2):\n",
    "    ''' Computes image autocorrelation. \n",
    "    Takes as input:\n",
    "        image: 2D image\n",
    "        filter: Boolean, if true will filter image with uniform filter\n",
    "        filtersize: size for uniform filtering\n",
    "    Returns:\n",
    "        corr_im: the image autocorrelation (this will be same size as image)\n",
    "        rav_corr: radially averaged image autocorrelation '''\n",
    "    \n",
    "    ### Crop image\n",
    "    #image = im[:1440, :1440]    ###option to crop out any large noise features \n",
    "        \n",
    "    if filter:\n",
    "        image = image*1.0 - uf(image,filtersize)   ###uniform filter, removes background. (\"filtersize\" is pixel area)\n",
    "    if bin:\n",
    "        image = downscale_local_mean(image, (binsize,binsize), cval=1)   ### OPTIONAL (binning makes things run faster)\n",
    "    image = 1.0*image-image.mean() #subtract mean\n",
    "    image = image/image.std() #normalize by standard deviation\n",
    "    corr_im = abs(fftshift(ifft2(fft2(image)*np.conj(fft2(image)))))/(image.shape[0]*image.shape[1])\n",
    "    \n",
    "    ### radial average taken\n",
    "    rav_corr = newRadav(corr_im)\n",
    "    \n",
    "    ### return ONLY rav_corr **b/c I don't need corr_im** (change this if you do need corr_im) \n",
    "    return rav_corr\n",
    "\n",
    "###not sure if this function is necessary\n",
    "def filtimage(image, filtersize=80):\n",
    "    image = image*1.0 - uf(image,filtersize)\n",
    "    return image\n",
    "\n",
    "###define function to find characteristic correlation lengths of SIA curves \n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "\n",
    "one_over_e = 1.0/np.exp(1) #characteristic decay length 1/e = 0.36787944117144233... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate your data (tiff files) and choose where to save results     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "directory = \"Z\"\n",
    "exp = \"9-5-22_s1_theBigOne\"\n",
    "### \"data_dir\" is the pathway to the folder holding the tiff files to be analyzed \n",
    "data_dir = directory+\":\\\\Gregor L\\\\__Kai Colloids\\\\\"+exp+\"\\\\all tiff files\\\\\" \n",
    "### \"plot_saveto\" is the pathway to the folder where plots and results will be saved\n",
    "plot_saveto= directory+\":\\\\Gregor L\\\\__Kai Colloids\\\\SIA\\\\Data\\\\\"+exp+\"\\\\\"\n",
    "\n",
    "files = glob.glob(data_dir+\"*_t*\") ### this should generate an ordered list of files in \"data_dir\" which have \"_t\" in their name\n",
    "print(\"found %i files\" % len(files))\n",
    "for i,f in enumerate(files): print (' %i \\t %s' % (i, f.split('\\\\')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify pixel size, each condition (frame_names), and the time points of data collection (time_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_names = [\"1-3 kA-WT\", \"WT (no kA)\", \"EA KaiC\", \"AE KaiC\"]\n",
    "### array containing the name for each frame in a tiff file to be run \n",
    "### e.g. frame 1 is an image of the \"50% bKaiB\" condition, frame 2 is an image of the \"35% bKaiB\" condition, etc.\n",
    "#[\"EE KaiC (-kA)\", \"EA KaiC (-kA)\", \"WT KaiC (-kA)\", \"AE KaiC (-kA)\"]\n",
    "#[\"50% bKaiB\", \"35% bKaiB\", \"20% bKaiB\", \"AE (20% bKaiB)\"]\n",
    "#[\"1-3 kA-WT\", \"WT (no kA)\", \"EA KaiC\", \"AE KaiC\"]\n",
    "#[\"2-1 kA-KaiC\", \"1-3 kA-KaiC\", \"WT KaiC (-kA)\", \"AE KaiC (-kA)\"]\n",
    "\n",
    "time_array = [0.67, 3.67, 6.67, 10, 12.83, 17.67, 21.67, 24.92, 28.17, 45.67, 49.08, 69.25]\n",
    "### array containing the time points corresponding to consecutive tiff files\n",
    "### e.g. tiff files \"bottom_row_t1\", \"middle_row_t1\", and \"top_row_t1\" all correspond to t = 0.5 hrs, time_array[0]\n",
    "#[0.42, 4.75, 8.75, 12.75, 19.92, 22.92, 25.42]\n",
    "#[3.88, 21.17, 68]\n",
    "#[0.55, 3.32, 42.73]\n",
    "#[0.5, 4.5, 8, 12, 24]\n",
    "#[0.5, 3.5, 6.5, 9.3, 12.1, 15, 18, 21, 24.5, 27.3, 40.5, 44.3]  \n",
    "#s1 [0.5, 3.2, 17.75, 20, 22, 24.25, 26, 40.5, 44]\n",
    "#s2 [0.85, 3.5, 18, 20.4, 22.25, 24.5, 26.2, 41.75, 44.2]\n",
    "#s1 [0.67, 3.67, 6.67, 10, 12.83, 17.67, 21.67, 24.92, 28.17, 45.67, 49.08, 69.25]\n",
    "#s2 [1, 3.83, 6.83, 10.17, 13.08, 18, 21.92, 25.25, 28.67, 45.92, 49.33, 69.42]\n",
    "\n",
    "pixel_size = 0.364 # 4*0.091 = 0.364\n",
    "### pixel size (microns per pixel) of frames/ images in the tiff files --- 40x olympus objective => 0.091 um/px\n",
    "### IF 2x2 BINNING: multiply the original pixel size by 2^2 = 4, e.g. 4*(0.091 um/px) = 0.364 um/px\n",
    "\n",
    "eg_im= tiff_file.imread(files[0])\n",
    "print(\"tiff file dimensions: \"+ str(eg_im.shape))\n",
    "\n",
    "arr_length = int((len(files))/3)\n",
    "print(\"total number of time points: \"+ str(arr_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose size, font, and quality level (dpi_num) for the plots to be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = 10,10/1.618\n",
    "###  size of output figures\n",
    "font_size = 16\n",
    "### font size\n",
    "dpi_num = 800\n",
    "### image quality level (recommendation: 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Set up empty arrays to save results in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_array = [0.0] * int(arr_length)\n",
    "br_corr_rad_array = [0.0] * int(arr_length)\n",
    "mr_corr_rad_array = [0.0] * int(arr_length)\n",
    "tr_corr_rad_array = [0.0] * int(arr_length)\n",
    "\n",
    "br_plat_vals = [0.0] * int(arr_length)\n",
    "mr_plat_vals = [0.0] * int(arr_length)\n",
    "tr_plat_vals = [0.0] * int(arr_length)\n",
    "\n",
    "all_xvalues = [0.0] * int(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can run some SIA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key = 0\n",
    "### \"key\" specifies which frame of each tiff file will be analyzed (each frame of my tiff is for a different condition)\n",
    "### e.g. \"key = 0\" dictates that the first frame of each tiff file should be analyzed, tiff_file.imread(files[i],key=[key]\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('Reds') \n",
    "### cmap dictates the color gradient used in plots; options: 'Reds' 'Blues' 'Greens' 'Greys' ... \n",
    "\n",
    "t_f = True\n",
    "### If true, filters images to remove excess fluorescense background or other noise\n",
    "\n",
    "size = 600  \n",
    "### set filter size (pixel area used to estimate and remove average background pixel intensities, recommendation = 600\n",
    "\n",
    "plat_range_from = 85\n",
    "plat_range_to = 275\n",
    "### set the range for finding average autocorrelation values of long distance correlation plateaus\n",
    "\n",
    "### Set basic plot design \n",
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(figsize=(fig_size))\n",
    "ax.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "markerSize = 4\n",
    "\n",
    "cmap_num = (arr_length*2) - 2\n",
    "### cmap_num is used the when plotting each curve to adjust the color gradient according to the total number of time points\n",
    "### e.g. for i in range(arr_length): the color of each curve is determined by c=cmap(0.9-(i/cmap_num))\n",
    "\n",
    "### Set up more empty arrays to save results in\n",
    "mean_corr_rad_array  = [0.0] * int(len(files))\n",
    "std_error_array = [0.0] * int(len(files))\n",
    "all_plat_vals = np.zeros((3,1))\n",
    "avg_plats  = np.empty(arr_length)\n",
    "std_error_plats = np.empty(arr_length)\n",
    "\n",
    "for i in range(arr_length):    \n",
    "    frame_num = \"frame %i\" % int(key +1)    ### used in plot title to specify which frame of all tiff files was analyzed\n",
    "    data_file = str(time_array[i]) + \" hrs\" ### used in legend to show time points corresponding to each curve\n",
    "    \n",
    "### \"im_corr\" is the actual SIA function which filters, bins, and fourier transforms tiff images to generate SIA curves\n",
    "    br_corr_rad_array[i] = SIA(tiff_file.imread(files[i],key=[key]),filter=t_f,filtersize=size, bin=True, binsize=2)\n",
    "    mr_corr_rad_array[i] = SIA(tiff_file.imread(files[i+arr_length],key=[key]),filter=t_f,filtersize=size, bin=True, binsize=2)\n",
    "    tr_corr_rad_array[i] = SIA(tiff_file.imread(files[i+(arr_length*2)],key=[key]),filter=t_f,filtersize=size, bin=True, binsize=2)\n",
    "    \n",
    "### corresponding x-values calculated according to the length of a SIA curve array (br_corr_rad_array[0]) and pixel size\n",
    "    all_xvalues = np.arange(len(br_corr_rad_array[0]))*pixel_size\n",
    "    \n",
    "### this block calculates the average and std error of the 3 SIA curves (\"br_corr_rad_array[i]\", \"mr_corr_rad_array[i]\", and \n",
    "### \"tr_corr_rad_array[i]\") generated from the specified frame (key) of 3 tiff files corresponding to time point \"time_array[i]\"\n",
    "    all_ims = np.zeros((3,len(br_corr_rad_array[i])))\n",
    "    all_ims[0] = br_corr_rad_array[i]\n",
    "    all_ims[1] = mr_corr_rad_array[i]\n",
    "    all_ims[2] = tr_corr_rad_array[i]\n",
    "    mean_corr_rad_array[i] = all_ims.mean(axis=0)\n",
    "    std_error_array[i] = (all_ims.std(axis=0))/np.sqrt(3)\n",
    "    \n",
    "### plot the average SIA curves for each time point with the std error as error bars \n",
    "    plt.semilogx(all_xvalues, mean_corr_rad_array[i],'.',ms=markerSize,c=cmap(0.9-(i/cmap_num)),label=data_file)\n",
    "    ax.errorbar(all_xvalues, mean_corr_rad_array[i], std_error_array[i], fmt = 'none', elinewidth=0.9, c=cmap(0.9-(i/cmap_num))) \n",
    "    \n",
    "\n",
    "print(\"long length plateau range: from %5.3f to %5.3f\" %(all_xvalues[plat_range_from], all_xvalues[plat_range_to]))\n",
    "print(\"image size: %5.3f x %5.3f um, last x-value= %5.3f\" %(all_xvalues[-1], all_xvalues[-1], all_xvalues[-1]))\n",
    "\n",
    "### legend, labels and title for the plot \n",
    "### **note: the title determined in this code block carries over to all other plots unless otherwise specified \n",
    "plt.xlabel(\"Distance ($\\mu$m)\",fontsize=font_size)\n",
    "plt.ylabel(\"Autocorrelation\",fontsize=font_size) \n",
    "ax.legend(loc=0, markerscale=4.,fontsize=font_size-2)\n",
    "ax.tick_params(direction='in', which='both', labelsize=font_size)\n",
    "\n",
    "### x-axis limit is set slighlty larger than the image size, based on \"all_xvalues[-1]\" \n",
    "plt.xlim(0, all_xvalues[-1] +2) \n",
    "\n",
    "### set title\n",
    "if t_f == False:\n",
    "    fsize = \"None\"\n",
    "else:\n",
    "    fsize = str(size)\n",
    "title = frame_names[key] + ' (' + frame_num + ') filter= ' + fsize\n",
    "plt.title(title, fontsize=font_size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###save figure \n",
    "fig.savefig(plot_saveto+\"SIA avg w error for \"+title+\".jpg\", dpi=dpi_num)\n",
    "print(plot_saveto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we fit the SIA curves to a single exponential, fit equation: y = A(e^(-x/L1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(figsize=(fig_size))\n",
    "ax.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "markerSize = 6\n",
    "\n",
    "### Set up dictionary (\"results_dict\") and more empty arrays to save results in\n",
    "results_dict = {}\n",
    "results_dict[\"time array\"] = time_array\n",
    "row1_cl = [0.0] * arr_length\n",
    "row2_cl = [0.0] * arr_length\n",
    "row3_cl = [0.0] * arr_length\n",
    "\n",
    "### FIT PARAMETERS: adjust these to change the range for fitting attempts \n",
    "fit_start = 0\n",
    "fit_lim = -200 \n",
    "\n",
    "x_fit_lim = (all_xvalues[fit_lim])/2\n",
    "print(\"first x-value= %5.3f, start fits at %5.3f; fit until xlim = %5.3f\" %(all_xvalues[1], all_xvalues[fit_start], x_fit_lim))\n",
    "#print(\"Fits:\")\n",
    "\n",
    "for i in range(arr_length):     \n",
    "    full_filename = files[i]\n",
    "    time = str(time_array[i]) + \" hrs\" #\"time \"+(full_filename.split('\\\\')[-1])[12:-4]\n",
    "\n",
    "### load and plot the 3 original SIA curves corresponding to 3 tiff files associated with each time point \n",
    "    br_y_array = br_corr_rad_array[i]\n",
    "    mr_y_array = mr_corr_rad_array[i]\n",
    "    tr_y_array = tr_corr_rad_array[i]\n",
    "    plt.semilogy(all_xvalues,br_y_array,'.',ms=markerSize,c=cmap(0.9-(i/cmap_num)),label=time)\n",
    "    plt.semilogy(all_xvalues,mr_y_array,'.',ms=markerSize,c=cmap(0.9-(i/cmap_num)))\n",
    "    plt.semilogy(all_xvalues,tr_y_array,'.',ms=markerSize,c=cmap(0.9-(i/cmap_num)))\n",
    "    \n",
    "### x_fit_values has the same range as \"all_xvalues\", but includes more values to produce better fits\n",
    "    x_fit_values = np.linspace(all_xvalues[fit_start], x_fit_lim, 1000) \n",
    "\n",
    "### the \"curve_fit\" function from scipy does the initial fitting attempt\n",
    "    ###this section does an 18 degree polynomial fit of the SIA data (corr_rad_array) for each frame\n",
    "    br_fit = np.poly1d(np.polyfit(all_xvalues[fit_start:fit_lim], br_y_array[fit_start:fit_lim], 17))\n",
    "    mr_fit = np.poly1d(np.polyfit(all_xvalues[fit_start:fit_lim], mr_y_array[fit_start:fit_lim], 17))\n",
    "    tr_fit = np.poly1d(np.polyfit(all_xvalues[fit_start:fit_lim], tr_y_array[fit_start:fit_lim], 17))\n",
    "    \n",
    "    br_fit_values = br_fit(x_fit_values)\n",
    "    mr_fit_values = mr_fit(x_fit_values)\n",
    "    tr_fit_values = tr_fit(x_fit_values)\n",
    "    \n",
    "### plot curves based on the fits \n",
    "    plt.semilogy(x_fit_values, br_fit_values,'--',c=cmap(0.99-(i/cmap_num)))\n",
    "    plt.semilogy(x_fit_values, mr_fit_values,'--',c=cmap(0.99-(i/cmap_num)))\n",
    "    plt.semilogy(x_fit_values, tr_fit_values,'--',c=cmap(0.99-(i/cmap_num)))\n",
    "    \n",
    "    ### find the x-value corresponding to the fit line y-value nearest to 1/e\n",
    "    br_nearest = find_nearest(br_fit_values, one_over_e)\n",
    "    br_corr_length = float(x_fit_values[np.where(br_fit_values == br_nearest)])\n",
    "    \n",
    "    mr_nearest = find_nearest(mr_fit_values, one_over_e)\n",
    "    mr_corr_length = float(x_fit_values[np.where(mr_fit_values == mr_nearest)])\n",
    "    \n",
    "    tr_nearest = find_nearest(tr_fit_values, one_over_e)\n",
    "    tr_corr_length = float(x_fit_values[np.where(tr_fit_values == tr_nearest)])\n",
    "    \n",
    "### saving all data & results to results dictionary \n",
    "    row3_cl[i] = br_corr_length\n",
    "    row2_cl[i] = mr_corr_length\n",
    "    row1_cl[i] = tr_corr_length\n",
    "    print(\"br_yarray[0] = %5.3f, br_yarray[1] = %5.3f, br_fit_values[0] = %5.3f, br_fit_values[1] = %5.3f\" %(br_y_array[0],\n",
    "                                                                                                         br_y_array[1],\n",
    "                                                                                                         br_fit_values[0], \n",
    "                                                                                                         br_fit_values[1]))\n",
    "    \n",
    "plt.xlabel(\"Distance ($\\mu$m)\",fontsize=font_size)\n",
    "plt.ylabel(\"Autocorrelation\",fontsize=font_size)\n",
    "ax.legend(loc=0, markerscale=2.,fontsize=font_size-3)\n",
    "plt.axhline(y=one_over_e, color='r', linestyle='-', label='1/e')\n",
    "plt.ylim(0.1, 1.1)\n",
    "#plt.ylim(0.01, 1.01)\n",
    "plt.xlim(0, 5)\n",
    "#plt.xlim(0, all_xvalues[fit_lim]+5) ## (0.091 um/px) * (1440 px) = 131.04 um --> =size of image x-axis in microns\n",
    "\n",
    "title_plus = title + \"; fits range (%5.3f um, %5.3f um)\" %(all_xvalues[fit_start], x_fit_lim)\n",
    "plt.title(title_plus, fontsize=font_size -4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###save figure\n",
    "fig.savefig(plot_saveto+\"SIA fits for \"+title_plus+\".jpg\", dpi=dpi_num)\n",
    "###save dictionary results \n",
    "file_to_write = open(plot_saveto+ \"SIA results for \"+title+\".p\", \"wb\")\n",
    "#file_to_write = open(plot_saveto+ \"SIA results for \"+title_plus+\".p\", \"wb\")\n",
    "pickle.dump(results_dict, file_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine and plot the results of fitting SIA curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### set up empty arrays\n",
    "all_cl = np.zeros((3,len(row1_cl)))\n",
    "avg_cl = np.empty(len(row1_cl))\n",
    "stderror_cl = np.empty(len(row1_cl))\n",
    "\n",
    "### find average values and std. error for L1 correlation lengths based on the fits \n",
    "for i in range(len(row1_cl)):\n",
    "    all_cl[0,i] = row1_cl[i]\n",
    "    all_cl[1,i] = row2_cl[i]\n",
    "    all_cl[2,i] = row3_cl[i]\n",
    "    #print(all_L1.mean(axis=0))\n",
    "avg_cl = all_cl.mean(axis=0)\n",
    "stderror_cl = all_cl.std(axis=0)/np.sqrt(3)\n",
    "print(stderror_cl)\n",
    "print(all_cl)\n",
    "\n",
    "### plot average values and std. error for L1 correlation lengths\n",
    "fig, ax = plt.subplots(figsize=(fig_size))\n",
    "markerSize = 8\n",
    "\n",
    "for i in range(int(len(time_array))):\n",
    "    plt.plot(time_array[i], avg_cl[i],'s', ms=markerSize, c=cmap(0.9-(i/cmap_num)))\n",
    "    ax.errorbar(time_array[i], avg_cl[i], yerr = stderror_cl[i], fmt = 'none', \n",
    "                ecolor=cmap(0.9-(i/cmap_num)), capsize=10)\n",
    "    \n",
    "plt.xlabel(\"time (hrs after adding KaiC)\",fontsize=font_size)\n",
    "plt.ylabel(\"correlation length ($\\mu$m)\",fontsize=font_size)\n",
    "ax.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "plt.ylim(0,12)\n",
    "title_plus = title + \"; fits range (%5.3f um, %5.3f um)\" %(all_xvalues[fit_start], x_fit_lim)\n",
    "plt.title(title_plus, fontsize=font_size -4)\n",
    "plt.show()\n",
    "\n",
    "### save plot\n",
    "fig.savefig(plot_saveto+\"1-over-e correlation lengths for \"+title_plus+\".jpg\", dpi=dpi_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set p_range to the range of long range autocorrelation plateau values, use for saving data in CSV files\n",
    "p_range = '(%5.1f, %5.1f)' %(all_xvalues[plat_range_from], all_xvalues[plat_range_to])\n",
    "print(p_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all results to 3 seperate CSV files --> use for plotting in origin later on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### first CSV file: save fit parameters (coefficient 'A' and correlation length 'L1'), and avg autocorrelation plateau values\n",
    "csv_data_file = exp+\" --threshold results (for box plots\"+\", \"+ \"filter= \" + fsize+\")\"+\".csv\"\n",
    "data_file_exists = os.path.isfile(saveto_dir+csv_data_file)\n",
    "if data_file_exists:\n",
    "    print(\"Data file already exists.\")\n",
    "    with open(saveto_dir + csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow([frame_names[key], '', 'filter= ' + fsize])\n",
    "        writer.writerow(['',\n",
    "                         'clusters detected','','','',\n",
    "                         'max cluster size','','','',\n",
    "                         'mean cluster size','','','',\n",
    "                         'avg clustering of a pixel',''])\n",
    "        writer.writerow(['time (hrs)',\n",
    "                         'row1, 2, 3','','avg','std error',])\n",
    "        for i in range(len(time_array)):\n",
    "            writer.writerow([time_array[i], row1_cl[i], '','','' ])\n",
    "            writer.writerow([time_array[i], row2_cl[i], '','','' ])\n",
    "            writer.writerow([time_array[i], row3_cl[i], '', , avg_cl[i], stderror_cl[i] ])\n",
    "            writer.writerow([time_array[i], \n",
    "                             row2_results[i][0], '','','',\n",
    "                             row2_results[i][1], '','','',\n",
    "                             row2_results[i][2], '','','',\n",
    "                             row2_results[i][3], ''])\n",
    "            writer.writerow([time_array[i], \n",
    "                             row3_results[i][0], avg_cnums[i], stderror_cnums[i], '',\n",
    "                             row3_results[i][1], avg_max_size[i], stderror_max_size[i], '',\n",
    "                             row3_results[i][2], avg_mean_size[i], stderror_mean_size[i], '', \n",
    "                             row3_results[i][3], '',])\n",
    "        writer.writerow([''])\n",
    "    f.close()\n",
    "    print(\"Results appended to file.\")\n",
    "else:\n",
    "    print(\"Data file does NOT exist.\")\n",
    "    header = []\n",
    "    np.savetxt(saveto_dir+csv_data_file, header, fmt=\"%s\", delimiter=',')\n",
    "    with open(saveto_dir + csv_data_file,'a', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow([condition, '', stop_filter_index])\n",
    "        writer.writerow(['',\n",
    "                         'clusters detected','','','',\n",
    "                         'max cluster size','','','',\n",
    "                         'mean cluster size','','','',\n",
    "                         'avg clustering of a pixel',''])\n",
    "        writer.writerow(['time (hrs)',\n",
    "                         'row1, 2, 3','avg','std error','',\n",
    "                         'row1, 2, 3','avg','std error','',\n",
    "                         'row1, 2, 3','avg','std error','',\n",
    "                         'row1, 2, 3',''])\n",
    "        for i in range(len(time_array)):\n",
    "            writer.writerow([time_array[i], \n",
    "                             row1_results[i][0], '','','',\n",
    "                             row1_results[i][1], '','','',\n",
    "                             row1_results[i][2], '','','',\n",
    "                             row1_results[i][3], '',])\n",
    "            writer.writerow([time_array[i], \n",
    "                             row2_results[i][0], '','','',\n",
    "                             row2_results[i][1], '','','',\n",
    "                             row2_results[i][2], '','','',\n",
    "                             row2_results[i][3], '',])\n",
    "            writer.writerow([time_array[i], \n",
    "                             row3_results[i][0], avg_cnums[i], stderror_cnums[i], '',\n",
    "                             row3_results[i][1], avg_max_size[i], stderror_max_size[i], '',\n",
    "                             row3_results[i][2], avg_mean_size[i], stderror_mean_size[i], '', \n",
    "                             row3_results[i][3], '',])\n",
    "        writer.writerow([''])\n",
    "    f.close()\n",
    "    print(\"New csv created, results appended to file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First round complete! \n",
    "## Now we can scroll back to the top and change the 'key' variable to run through the next frame (i.e. condition) of all tiff files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
